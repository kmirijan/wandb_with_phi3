{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/edu/blob/main/llm-apps-course/notebooks/02.%20Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{llmapps-generation} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "<!--- @wandbcode{llmapps-generation} -->\n",
    "\n",
    "In this notebook we will dive deeper on prompting the model by passing a better context by using available data from users questions and using the documentation files to generate better answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -Uqqq rich openai==0.27.2 tiktoken wandb tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "\n",
    "from rich.markdown import Markdown\n",
    "import pandas as pd\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential, # for exponential backoff\n",
    ")  \n",
    "import wandb\n",
    "from wandb.integration.openai import autolog\n",
    "\n",
    "import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files on colab\n",
    "# if not Path(\"examples.txt\").exists():\n",
    "#     !wget https://raw.githubusercontent.com/wandb/edu/main/llm-apps-course/notebooks/{examples,prompt_template,system_template}.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need an OpenAI API key to run this notebook. You can get one [here](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "#   if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "#     print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "#   os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "#   openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "# print(\"OpenAI API key configured\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key.open_ai_key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enable W&B autologging to track our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkmirijan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kmirijan/wandb/wandb-llm-apps-course/llm-apps-course/notebooks/wandb/run-20241129_090511-2ugzwzbk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kmirijan/llmapps/runs/2ugzwzbk' target=\"_blank\">stellar-frog-8</a></strong> to <a href='https://wandb.ai/kmirijan/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kmirijan/llmapps' target=\"_blank\">https://wandb.ai/kmirijan/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kmirijan/llmapps/runs/2ugzwzbk' target=\"_blank\">https://wandb.ai/kmirijan/llmapps/runs/2ugzwzbk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start logging to W&B\n",
    "autolog({\"project\":\"llmapps\", \"job_type\": \"generation\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating synthetic support questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add a retry behavior in case we hit the API rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_NAME = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"Can you help me understand how to correctly log food portions in the Weight &amp; Balance app to ensure accurate      \n",
       "tracking of my daily intake?\"                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"Can you help me understand how to correctly log food portions in the Weight & Balance app to ensure accurate      \n",
       "tracking of my daily intake?\"                                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sure! Here's a support question that a user might ask related to weight and balance calculations for aircraft:     \n",
       "\n",
       "\"How do I account for the weight distribution of passengers and cargo when calculating the center of gravity for my\n",
       "aircraft?\"                                                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sure! Here's a support question that a user might ask related to weight and balance calculations for aircraft:     \n",
       "\n",
       "\"How do I account for the weight distribution of passengers and cargo when calculating the center of gravity for my\n",
       "aircraft?\"                                                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"How can I accurately track and manage my daily water intake using the W&amp;B app?\"                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"How can I accurately track and manage my daily water intake using the W&B app?\"                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"Can you provide guidance on how to properly interpret the weight and balance calculations for my aircraft?\"       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"Can you provide guidance on how to properly interpret the weight and balance calculations for my aircraft?\"       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\"What is the maximum weight limit for carry-on baggage on your flights?\"                                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\"What is the maximum weight limit for carry-on baggage on your flights?\"                                           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"Generate a support question from a W&B user\"\n",
    "\n",
    "def generate_and_print(system_prompt, user_prompt, n=5):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    responses = completion_with_backoff(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "        n = n,\n",
    "        )\n",
    "    for response in responses.choices:\n",
    "        generation = response.message.content\n",
    "        display(Markdown(generation))\n",
    "    \n",
    "generate_and_print(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read some user submitted queries from the file `examples.txt`. This file contains multiline questions separated by tabs (`\\t`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if examples.txt is present, download if not\n",
    "# if not Path(\"examples.txt\").exists():\n",
    "#     !wget https://raw.githubusercontent.com/wandb/edu/main/llm-apps-course/notebooks/examples.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'We have 228 real queries:'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sample one: \"can you tell me how to use W&amp;B sweep together with pytorch's DistributedDataParallel\"                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sample one: \"can you tell me how to use W&B sweep together with pytorch's DistributedDataParallel\"                 \n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delimiter = \"\\t\" # tab separated queries\n",
    "with open(\"examples.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "    real_queries = data.split(delimiter)\n",
    "\n",
    "pprint(f\"We have {len(real_queries)} real queries:\")  \n",
    "Markdown(f\"Sample one: \\n\\\"{random.choice(real_queries)}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use those real user questions to guide our model to produce synthetic questions like those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user Below you will find a few examples of real user queries: how do I fix  \n",
       "an error with wandb Table construction from pandas dataframe: TypeError: Data row contained incompatible types can \n",
       "i use wandb with slurm? I'm getting a \"Importing a module script failed. An application error occurred.\" error on a\n",
       "computer, but the same link works on my phone. Let's start!                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generate a support question from a W&B user Below you will find a few examples of real user queries: how do I fix  \n",
       "an error with wandb Table construction from pandas dataframe: TypeError: Data row contained incompatible types can \n",
       "i use wandb with slurm? I'm getting a \"Importing a module script failed. An application error occurred.\" error on a\n",
       "computer, but the same link works on my phone. Let's start!                                                        \n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_few_shot_prompt(queries, n=3):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"Below you will find a few examples of real user queries:\\n\"\n",
    "    for _ in range(n):\n",
    "        prompt += random.choice(queries) + \"\\n\"\n",
    "    prompt += \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "generation_prompt = generate_few_shot_prompt(real_queries)\n",
    "Markdown(generation_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI `Chat` models are really good at following instructions with a few examples. Let's see how it does here. This is going to use some context from the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sure! Here's another example of a support question from a W&amp;B user:                                                \n",
       "\n",
       "\"I am trying to use W&amp;B's sweep functionality for hyperparameter optimization, but I keep encountering errors when \n",
       "setting up the configurations for my sweep. Can you provide guidance on how to troubleshoot this issue?\"           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Sure! Here's another example of a support question from a W&B user:                                                \n",
       "\n",
       "\"I am trying to use W&B's sweep functionality for hyperparameter optimization, but I keep encountering errors when \n",
       "setting up the configurations for my sweep. Can you provide guidance on how to troubleshoot this issue?\"           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How do I resolve the issue of my wandb Table construction from pandas dataframe resulting in a TypeError: Data row \n",
       "contained incompatible types?                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How do I resolve the issue of my wandb Table construction from pandas dataframe resulting in a TypeError: Data row \n",
       "contained incompatible types?                                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I resolve the issue of my wandb Table construction from a pandas dataframe resulting in the error:         \n",
       "TypeError: Data row contained incompatible types?                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How can I resolve the issue of my wandb Table construction from a pandas dataframe resulting in the error:         \n",
       "TypeError: Data row contained incompatible types?                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How can I track multiple metrics from different sources concurrently in WandB?                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How can I track multiple metrics from different sources concurrently in WandB?                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">How do I troubleshoot the \"ValueError: Column name collides with a field name: <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">step</span>\" issue when trying to log data \n",
       "using W&amp;B in my Python script?                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "How do I troubleshoot the \"ValueError: Column name collides with a field name: \u001b[1;36;40mstep\u001b[0m\" issue when trying to log data \n",
       "using W&B in my Python script?                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_print(system_prompt, user_prompt=generation_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Context & Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to find all the markdown files in a directory and return it's content and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if directory exists, if not, create it and download the files, e.g if running in colab\n",
    "# if not os.path.exists(\"../docs_sample/\"):\n",
    "#   !git clone https://github.com/wandb/edu.git\n",
    "#   !cp -r edu/llm-apps-course/docs_sample ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_md_files(directory):\n",
    "    \"Find all markdown files in a directory and return their content and path\"\n",
    "    md_files = []\n",
    "    for file in Path(directory).rglob(\"*.md\"):\n",
    "        with open(file, 'r', encoding='utf-8') as md_file:\n",
    "            content = md_file.read()\n",
    "        md_files.append((file.relative_to(directory), content))\n",
    "    return md_files\n",
    "\n",
    "documents = find_md_files('../docs_sample/')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the documents are not too long for our context window. We need to compute the number of tokens in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[803, 1644, 2596, 2093, 365, 1206, 2940, 4179, 2529, 537, 956]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "tokens_per_document = [len(tokenizer.encode(document)) for _, document in documents]\n",
    "pprint(tokens_per_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them are too long - instead of using entire documents, we'll extract a random chunk from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a random chunk from a document\n",
    "def extract_random_chunk(document, max_tokens=512):\n",
    "    tokens = tokenizer.encode(document)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return document\n",
    "    start = random.randint(0, len(tokens) - max_tokens)\n",
    "    end = start + max_tokens\n",
    "    return tokenizer.decode(tokens[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use that extracted chunk to create a question that can be answered by the document. This way we can generate questions that our current documentation is capable of answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk):\n",
    "    prompt = \"Generate a support question from a W&B user\\n\" +\\\n",
    "        \"The question should be answerable by provided fragment of W&B documentation.\\n\" +\\\n",
    "        \"Below you will find a fragment of W&B documentation:\\n\" +\\\n",
    "        chunk + \"\\n\" +\\\n",
    "        \"Let's start!\"\n",
    "    return prompt\n",
    "\n",
    "chunk = extract_random_chunk(documents[0][1])\n",
    "generation_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Generate a support question from a W&amp;B user The question should be answerable by provided fragment of W&amp;B          \n",
       "documentation. Below you will find a fragment of W&amp;B documentation: to store whatever files are produced from the  \n",
       "serialization process provided by your modeling library (eg <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">PyTorch</span></a> &amp; <a href=\"https://www.tensorflow.org/guide/keras/save_and_serialize\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Keras</span></a>).                                      \n",
       "\n",
       "Furthermore, a <span style=\"font-weight: bold\">Model Artifact</span> is a sequence of Model Versions. Model Artifact can <span style=\"font-weight: bold\">alias</span> specific versions so that  \n",
       "downstream consumers can pin to such aliases. It is extremely common for a W&amp;B Run to produce many versions of a   \n",
       "model while training (periodically saving checkpoints). Using this approach, each individual model being trained by\n",
       "the Run corresponds to its own Model Artifact, and each checkpoint corresponds to its own Model Version of the     \n",
       "respective Model Artifact.                                                                                         \n",
       "\n",
       "View an <a href=\"https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/mnist-zws7gt0n\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; text-decoration: underline\">example Model Artifact -&gt;</span></a>                                                                                  \n",
       "\n",
       "🌆 <a href=\"@site/static/images/models/mr1c.png\" target=\"_blank\">mr1c.png</a>                                                                                                                    \n",
       "\n",
       "Finally, a <span style=\"font-weight: bold\">Registered Model</span> is a set of links to Model Versions. A Registered Model can be accessed exactly like   \n",
       "Model Artifacts (identified by <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">[[entityName/]/projectName]/registeredModelName:alias</span>), however it acts more like a \n",
       "folder of \"bookmarks\" - where each \"version\" of a Registered Model is actually a link to an Model Version belonging\n",
       "to a Model Artifact of the same type. A Model Version may be linked to any number of Registered Models. Typically  \n",
       "you will create a Registered Model for each of your use cases / modeling tasks and use aliases like \"production\" or\n",
       "\"staging\" to denote versions with special purposes.                                                                \n",
       "\n",
       "View an <a href=\"https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/MNIST%20Grayscale%2028x28\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; text-decoration: underline\">example Registered Model -&gt;</span></a>                                                                                \n",
       "\n",
       "🌆 <a href=\"/images/models/diagram_doc.png\" target=\"_blank\">diagram_doc.png</a>                                                                                                                    \n",
       "\n",
       "While developing an ML Model, you will likely have dozens, hundreds, or even thousands of Runs which produce Model \n",
       "Versions - they may come from notebooks, remote training jobs, CI/CD pipelines, etc... Most likely, not all of     \n",
       "those models are great; often you are iterating on scripts, parameters, architectures, preprocessing logic and     \n",
       "more. The separation of Artifacts and Registered Models allows you to produce a massive number of Artifacts (think \n",
       "of them like \"draft models\"), and periodically <span style=\"font-style: italic\">link</span> your high performing versions to a the curated Registered      \n",
       "Model. Then use Let's start!                                                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Generate a support question from a W&B user The question should be answerable by provided fragment of W&B          \n",
       "documentation. Below you will find a fragment of W&B documentation: to store whatever files are produced from the  \n",
       "serialization process provided by your modeling library (eg \u001b]8;id=207392;https://pytorch.org/tutorials/beginner/saving_loading_models.html\u001b\\\u001b[4;34mPyTorch\u001b[0m\u001b]8;;\u001b\\ & \u001b]8;id=799415;https://www.tensorflow.org/guide/keras/save_and_serialize\u001b\\\u001b[4;34mKeras\u001b[0m\u001b]8;;\u001b\\).                                      \n",
       "\n",
       "Furthermore, a \u001b[1mModel Artifact\u001b[0m is a sequence of Model Versions. Model Artifact can \u001b[1malias\u001b[0m specific versions so that  \n",
       "downstream consumers can pin to such aliases. It is extremely common for a W&B Run to produce many versions of a   \n",
       "model while training (periodically saving checkpoints). Using this approach, each individual model being trained by\n",
       "the Run corresponds to its own Model Artifact, and each checkpoint corresponds to its own Model Version of the     \n",
       "respective Model Artifact.                                                                                         \n",
       "\n",
       "View an \u001b]8;id=492290;https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/mnist-zws7gt0n\u001b\\\u001b[1;4;34mexample Model Artifact ->\u001b[0m\u001b]8;;\u001b\\                                                                                  \n",
       "\n",
       "🌆 \u001b]8;id=197299;@site/static/images/models/mr1c.png\u001b\\mr1c.png\u001b]8;;\u001b\\                                                                                                                    \n",
       "\n",
       "Finally, a \u001b[1mRegistered Model\u001b[0m is a set of links to Model Versions. A Registered Model can be accessed exactly like   \n",
       "Model Artifacts (identified by \u001b[1;36;40m[[entityName/]/projectName]/registeredModelName:alias\u001b[0m), however it acts more like a \n",
       "folder of \"bookmarks\" - where each \"version\" of a Registered Model is actually a link to an Model Version belonging\n",
       "to a Model Artifact of the same type. A Model Version may be linked to any number of Registered Models. Typically  \n",
       "you will create a Registered Model for each of your use cases / modeling tasks and use aliases like \"production\" or\n",
       "\"staging\" to denote versions with special purposes.                                                                \n",
       "\n",
       "View an \u001b]8;id=635987;https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/MNIST%20Grayscale%2028x28\u001b\\\u001b[1;4;34mexample Registered Model ->\u001b[0m\u001b]8;;\u001b\\                                                                                \n",
       "\n",
       "🌆 \u001b]8;id=704886;/images/models/diagram_doc.png\u001b\\diagram_doc.png\u001b]8;;\u001b\\                                                                                                                    \n",
       "\n",
       "While developing an ML Model, you will likely have dozens, hundreds, or even thousands of Runs which produce Model \n",
       "Versions - they may come from notebooks, remote training jobs, CI/CD pipelines, etc... Most likely, not all of     \n",
       "those models are great; often you are iterating on scripts, parameters, architectures, preprocessing logic and     \n",
       "more. The separation of Artifacts and Registered Models allows you to produce a massive number of Artifacts (think \n",
       "of them like \"draft models\"), and periodically \u001b[3mlink\u001b[0m your high performing versions to a the curated Registered      \n",
       "Model. Then use Let's start!                                                                                       \n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(generation_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate 3 possible questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Certainly! A support question related to the provided fragment of W&amp;B documentation could be:                      \n",
       "\n",
       "\"How can I link my high-performing model versions to a curated Registered Model in Weights &amp; Biases?\"              \n",
       "\n",
       "This question can be answered by referring to the concept described in the documentation where a Registered Model  \n",
       "is defined as a set of links to Model Versions. It explains that you can create a Registered Model for each of your\n",
       "use cases or modeling tasks and use aliases like \"production\" or \"staging\" to denote versions with specific        \n",
       "purposes. Each \"version\" of a Registered Model is actually a link to a Model Version belonging to a Model Artifact \n",
       "of the same type.                                                                                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Certainly! A support question related to the provided fragment of W&B documentation could be:                      \n",
       "\n",
       "\"How can I link my high-performing model versions to a curated Registered Model in Weights & Biases?\"              \n",
       "\n",
       "This question can be answered by referring to the concept described in the documentation where a Registered Model  \n",
       "is defined as a set of links to Model Versions. It explains that you can create a Registered Model for each of your\n",
       "use cases or modeling tasks and use aliases like \"production\" or \"staging\" to denote versions with specific        \n",
       "purposes. Each \"version\" of a Registered Model is actually a link to a Model Version belonging to a Model Artifact \n",
       "of the same type.                                                                                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">User question: How can I store and manage different versions of models produced during training in W&amp;B?            \n",
       "\n",
       "Answer: You can store different versions of models produced during training in W&amp;B using Model Artifacts and       \n",
       "Registered Models. A Model Artifact is a sequence of Model Versions, where each checkpoint corresponds to its own  \n",
       "Model Version of the respective Model Artifact. On the other hand, a Registered Model is a set of links to Model   \n",
       "Versions, acting like a folder of bookmarks. You can create a Registered Model for each of your use cases or       \n",
       "modeling tasks and use aliases like \"production\" or \"staging\" to denote versions with special purposes. This allows\n",
       "you to produce a massive number of Artifacts (like \"draft models\") and periodically link your high-performing      \n",
       "versions to a curated Registered Model.                                                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "User question: How can I store and manage different versions of models produced during training in W&B?            \n",
       "\n",
       "Answer: You can store different versions of models produced during training in W&B using Model Artifacts and       \n",
       "Registered Models. A Model Artifact is a sequence of Model Versions, where each checkpoint corresponds to its own  \n",
       "Model Version of the respective Model Artifact. On the other hand, a Registered Model is a set of links to Model   \n",
       "Versions, acting like a folder of bookmarks. You can create a Registered Model for each of your use cases or       \n",
       "modeling tasks and use aliases like \"production\" or \"staging\" to denote versions with special purposes. This allows\n",
       "you to produce a massive number of Artifacts (like \"draft models\") and periodically link your high-performing      \n",
       "versions to a curated Registered Model.                                                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">User support question: How can I create and manage Registered Models in W&amp;B for my machine learning projects?      \n",
       "\n",
       "Answer: You can create and manage Registered Models in W&amp;B by using aliases to denote versions with special        \n",
       "purposes, such as \"production\" or \"staging.\" Registered Models are sets of links to Model Versions, acting like    \n",
       "folders of bookmarks that are linked to Model Versions belonging to Model Artifacts. Each Model Version can be     \n",
       "linked to any number of Registered Models, allowing you to organize and curate your models effectively.            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "User support question: How can I create and manage Registered Models in W&B for my machine learning projects?      \n",
       "\n",
       "Answer: You can create and manage Registered Models in W&B by using aliases to denote versions with special        \n",
       "purposes, such as \"production\" or \"staging.\" Registered Models are sets of links to Model Versions, acting like    \n",
       "folders of bookmarks that are linked to Model Versions belonging to Model Artifacts. Each Model Version can be     \n",
       "linked to any number of Registered Models, allowing you to organize and curate your models effectively.            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_and_print(system_prompt, generation_prompt, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, sometimes the generation contains an intro phrase like: \"Sure, here's a support question based on the documentation:\", we may want to put some instructions to avoid this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Level 5 prompt\n",
    "\n",
    "Complex directive that includes the following:\n",
    "- Description of high-level goal\n",
    "- A detailed bulleted list of sub-tasks\n",
    "- An explicit statement asking LLM to explain its own output\n",
    "- A guideline on how LLM output will be evaluated\n",
    "- Few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use GPT4 from here, as it gives better answers and abides to instructions better\n",
    "MODEL_NAME = \"gpt-4\"\n",
    "# MODEL_NAME = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read system_template.txt file into an f-string\n",
    "with open(\"system_template.txt\", \"r\") as file:\n",
    "    system_prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You are a creative assistant with the goal to generate a synthetic dataset of Weights &amp; Biases (W&amp;B) user          \n",
       "questions. W&amp;B users are asking these questions to a bot, so they don't know the answer and their questions are    \n",
       "grounded in what they're trying to achieve. We are interested in questions that can be answered by W&amp;B             \n",
       "documentation. But the users don't have access to this documentation, so you need to imagine what they're trying to\n",
       "do and use according language.                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "You are a creative assistant with the goal to generate a synthetic dataset of Weights & Biases (W&B) user          \n",
       "questions. W&B users are asking these questions to a bot, so they don't know the answer and their questions are    \n",
       "grounded in what they're trying to achieve. We are interested in questions that can be answered by W&B             \n",
       "documentation. But the users don't have access to this documentation, so you need to imagine what they're trying to\n",
       "do and use according language.                                                                                     \n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prompt_template.txt file into an f-string\n",
    "with open(\"prompt_template.txt\", \"r\") as file:\n",
    "    prompt_template = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "{QUESTIONS}                                                                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "In the next step, you will read a fragment of W&amp;B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "{CHUNK}                                                                                                            \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how realistic is that this question will come from a real user one day?                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>is this question about W&amp;B?                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>can the question be answered using the W&amp;B document fragment above?                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>impersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "{QUESTIONS}                                                                                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "In the next step, you will read a fragment of W&B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "{CHUNK}                                                                                                            \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0mhow realistic is that this question will come from a real user one day?                                         \n",
       "\u001b[1;33m • \u001b[0mis this question about W&B?                                                                                     \n",
       "\u001b[1;33m • \u001b[0mcan the question be answered using the W&B document fragment above?                                             \n",
       "\u001b[1;33m • \u001b[0mhow accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "\u001b[1;33m   \u001b[0mimpersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_prompt(chunk, n_questions=3):\n",
    "    questions = '\\n'.join(random.sample(real_queries, n_questions))\n",
    "    user_prompt = prompt_template.format(QUESTIONS=questions, CHUNK=chunk)\n",
    "    return user_prompt\n",
    "\n",
    "user_prompt = generate_context_prompt(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "The movement is choppy when I drag either horizontally or vertically the wandb table. It seems that it takes a long\n",
       "time loading blocks of data. Is there a way to accomodate this? How to export a single chart's data using the API? \n",
       "@wandbot (beta) While running a sweep, runs fail with FileNotFoundError(2, 'No such file or directory'). I took a  \n",
       "look at the logs for those exact runs, but there is no error to be found there. In addition, once that error       \n",
       "happened for a agent, it keeps happening over and over, even if the agent before had successful runs. How do I     \n",
       "debug this?                                                                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "In the next step, you will read a fragment of W&amp;B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "to store whatever files are produced from the serialization process provided by your modeling library (eg <a href=\"https://pytorch.org/tutorials/beginner/saving_loading_models.html\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">PyTorch</span></a> &amp;\n",
       "<a href=\"https://www.tensorflow.org/guide/keras/save_and_serialize\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; text-decoration: underline\">Keras</span></a>).                                                                                                            \n",
       "\n",
       "Furthermore, a <span style=\"font-weight: bold\">Model Artifact</span> is a sequence of Model Versions. Model Artifact can <span style=\"font-weight: bold\">alias</span> specific versions so that  \n",
       "downstream consumers can pin to such aliases. It is extremely common for a W&amp;B Run to produce many versions of a   \n",
       "model while training (periodically saving checkpoints). Using this approach, each individual model being trained by\n",
       "the Run corresponds to its own Model Artifact, and each checkpoint corresponds to its own Model Version of the     \n",
       "respective Model Artifact.                                                                                         \n",
       "\n",
       "View an <a href=\"https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/mnist-zws7gt0n\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; text-decoration: underline\">example Model Artifact -&gt;</span></a>                                                                                  \n",
       "\n",
       "🌆 <a href=\"@site/static/images/models/mr1c.png\" target=\"_blank\">mr1c.png</a>                                                                                                                    \n",
       "\n",
       "Finally, a <span style=\"font-weight: bold\">Registered Model</span> is a set of links to Model Versions. A Registered Model can be accessed exactly like   \n",
       "Model Artifacts (identified by <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">[[entityName/]/projectName]/registeredModelName:alias</span>), however it acts more like a \n",
       "folder of \"bookmarks\" - where each \"version\" of a Registered Model is actually a link to an Model Version belonging\n",
       "to a Model Artifact of the same type. A Model Version may be linked to any number of Registered Models. Typically  \n",
       "you will create a Registered Model for each of your use cases / modeling tasks and use aliases like \"production\" or\n",
       "\"staging\" to denote versions with special purposes.                                                                \n",
       "\n",
       "View an <a href=\"https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/MNIST%20Grayscale%2028x28\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold; text-decoration: underline\">example Registered Model -&gt;</span></a>                                                                                \n",
       "\n",
       "🌆 <a href=\"/images/models/diagram_doc.png\" target=\"_blank\">diagram_doc.png</a>                                                                                                                    \n",
       "\n",
       "While developing an ML Model, you will likely have dozens, hundreds, or even thousands of Runs which produce Model \n",
       "Versions - they may come from notebooks, remote training jobs, CI/CD pipelines, etc... Most likely, not all of     \n",
       "those models are great; often you are iterating on scripts, parameters, architectures, preprocessing logic and     \n",
       "more. The separation of Artifacts and Registered Models allows you to produce a massive number of Artifacts (think \n",
       "of them like \"draft models\"), and periodically <span style=\"font-style: italic\">link</span> your high performing versions to a the curated Registered      \n",
       "Model. Then use                                                                                                    \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">───────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span>\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how realistic is that this question will come from a real user one day?                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>is this question about W&amp;B?                                                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>can the question be answered using the W&amp;B document fragment above?                                             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>how accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>impersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Here are some examples of real user questions, you will be judged by how well you match this distribution.         \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "The movement is choppy when I drag either horizontally or vertically the wandb table. It seems that it takes a long\n",
       "time loading blocks of data. Is there a way to accomodate this? How to export a single chart's data using the API? \n",
       "@wandbot (beta) While running a sweep, runs fail with FileNotFoundError(2, 'No such file or directory'). I took a  \n",
       "look at the logs for those exact runs, but there is no error to be found there. In addition, once that error       \n",
       "happened for a agent, it keeps happening over and over, even if the agent before had successful runs. How do I     \n",
       "debug this?                                                                                                        \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "In the next step, you will read a fragment of W&B documentation. This will serve as inspiration for synthetic user \n",
       "question and the source of the answer. Here is the document fragment:                                              \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "to store whatever files are produced from the serialization process provided by your modeling library (eg \u001b]8;id=220838;https://pytorch.org/tutorials/beginner/saving_loading_models.html\u001b\\\u001b[4;34mPyTorch\u001b[0m\u001b]8;;\u001b\\ &\n",
       "\u001b]8;id=421084;https://www.tensorflow.org/guide/keras/save_and_serialize\u001b\\\u001b[4;34mKeras\u001b[0m\u001b]8;;\u001b\\).                                                                                                            \n",
       "\n",
       "Furthermore, a \u001b[1mModel Artifact\u001b[0m is a sequence of Model Versions. Model Artifact can \u001b[1malias\u001b[0m specific versions so that  \n",
       "downstream consumers can pin to such aliases. It is extremely common for a W&B Run to produce many versions of a   \n",
       "model while training (periodically saving checkpoints). Using this approach, each individual model being trained by\n",
       "the Run corresponds to its own Model Artifact, and each checkpoint corresponds to its own Model Version of the     \n",
       "respective Model Artifact.                                                                                         \n",
       "\n",
       "View an \u001b]8;id=98737;https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/mnist-zws7gt0n\u001b\\\u001b[1;4;34mexample Model Artifact ->\u001b[0m\u001b]8;;\u001b\\                                                                                  \n",
       "\n",
       "🌆 \u001b]8;id=714704;@site/static/images/models/mr1c.png\u001b\\mr1c.png\u001b]8;;\u001b\\                                                                                                                    \n",
       "\n",
       "Finally, a \u001b[1mRegistered Model\u001b[0m is a set of links to Model Versions. A Registered Model can be accessed exactly like   \n",
       "Model Artifacts (identified by \u001b[1;36;40m[[entityName/]/projectName]/registeredModelName:alias\u001b[0m), however it acts more like a \n",
       "folder of \"bookmarks\" - where each \"version\" of a Registered Model is actually a link to an Model Version belonging\n",
       "to a Model Artifact of the same type. A Model Version may be linked to any number of Registered Models. Typically  \n",
       "you will create a Registered Model for each of your use cases / modeling tasks and use aliases like \"production\" or\n",
       "\"staging\" to denote versions with special purposes.                                                                \n",
       "\n",
       "View an \u001b]8;id=639271;https://wandb.ai/timssweeney/model_management_docs_official_v0/artifacts/model/MNIST%20Grayscale%2028x28\u001b\\\u001b[1;4;34mexample Registered Model ->\u001b[0m\u001b]8;;\u001b\\                                                                                \n",
       "\n",
       "🌆 \u001b]8;id=812949;/images/models/diagram_doc.png\u001b\\diagram_doc.png\u001b]8;;\u001b\\                                                                                                                    \n",
       "\n",
       "While developing an ML Model, you will likely have dozens, hundreds, or even thousands of Runs which produce Model \n",
       "Versions - they may come from notebooks, remote training jobs, CI/CD pipelines, etc... Most likely, not all of     \n",
       "those models are great; often you are iterating on scripts, parameters, architectures, preprocessing logic and     \n",
       "more. The separation of Artifacts and Registered Models allows you to produce a massive number of Artifacts (think \n",
       "of them like \"draft models\"), and periodically \u001b[3mlink\u001b[0m your high performing versions to a the curated Registered      \n",
       "Model. Then use                                                                                                    \n",
       "\n",
       "\u001b[33m───────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
       "You will now generate a user question and corresponding answer based on the above document. First, explain the user\n",
       "context and what problems they might be trying to solve. Second, generate user question. Third, provide the        \n",
       "accurate and concise answer in markdown format to the user question using the documentation. You'll be evaluated   \n",
       "on:                                                                                                                \n",
       "\n",
       "\u001b[1;33m • \u001b[0mhow realistic is that this question will come from a real user one day?                                         \n",
       "\u001b[1;33m • \u001b[0mis this question about W&B?                                                                                     \n",
       "\u001b[1;33m • \u001b[0mcan the question be answered using the W&B document fragment above?                                             \n",
       "\u001b[1;33m • \u001b[0mhow accurate is the answer? Remember that users have different styles and can be imprecise. You are very good at\n",
       "\u001b[1;33m   \u001b[0mimpersonating them! Use the following format: CONTEXT: QUESTION: ANSWER: Let's start!                           \n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(documents, n_questions=3, n_generations=5):\n",
    "    questions = []\n",
    "    for _, document in documents:\n",
    "        chunk = extract_random_chunk(document)\n",
    "        user_prompt = generate_context_prompt(chunk, n_questions)\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "        response = completion_with_backoff(\n",
    "            model=MODEL_NAME,\n",
    "            messages=messages,\n",
    "            n = n_generations,\n",
    "            )\n",
    "        questions.extend([response.choices[i].message.content for i in range(n_generations)])\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A Note about the `system` role: For GPT4 based pipelines you probably want to move some part of the context prompt to the `system` context. As we are using `gpt3.5-turbo` here, you can put the instruction on the user prompt, you can read more about this on [OpenAI docs here](https://platform.openai.com/docs/guides/chat/instructing-chat-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse model generation and extract CONTEXT, QUESTION and ANSWER\n",
    "def parse_generation(generation):\n",
    "    lines = generation.split(\"\\n\")\n",
    "    context = []\n",
    "    question = []\n",
    "    answer = []\n",
    "    flag = None\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"CONTEXT:\" in line:\n",
    "            flag = \"context\"\n",
    "            line = line.replace(\"CONTEXT:\", \"\").strip()\n",
    "        elif \"QUESTION:\" in line:\n",
    "            flag = \"question\"\n",
    "            line = line.replace(\"QUESTION:\", \"\").strip()\n",
    "        elif \"ANSWER:\" in line:\n",
    "            flag = \"answer\"\n",
    "            line = line.replace(\"ANSWER:\", \"\").strip()\n",
    "\n",
    "        if flag == \"context\":\n",
    "            context.append(line)\n",
    "        elif flag == \"question\":\n",
    "            question.append(line)\n",
    "        elif flag == \"answer\":\n",
    "            answer.append(line)\n",
    "\n",
    "    context = \"\\n\".join(context)\n",
    "    question = \"\\n\".join(question)\n",
    "    answer = \"\\n\".join(answer)\n",
    "    return context, question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\nA user has just started using Weights & Biases. They have hundreds of runs that have resulted in different versions of their ML models. They want to understand more about managing model versions and how can they categorize all the different model versions (each suited for a unique use case) in W&B. \\n',\n",
       " '\\nI have a bunch of ML model versions bottled up from numerous runs. How can I manage and categorize these in different sets, each tagged for a particular use case, in Weights & Biases?\\n',\n",
       " '\\nIn Weights & Biases, you can manage and categorize your model versions using **Model Artifacts** and **Registered Models**.\\n\\nA **Model Artifact** is a sequence of Model Versions produced from different training runs. Each individual model being trained corresponds to its own Model Artifact, and each checkpoint during the training corresponds to a Model Version of that respective Model Artifact. You can also **alias** specific versions so that you can easily pin and track them. \\n\\nA **Registered Model**, on the other hand, acts like a folder of \"bookmarks\" — each \"version\" of a Registered Model is actually a link to a Model Version. So, you can categorize different model versions under different Registered Models. Each Model Version may be linked to any number of Registered Models. Typically, you would create a Registered Model for each of your use cases or modeling tasks and use aliases like \"production\" or \"staging\" to denote versions with special purposes. \\n\\nThus, using a combination of Model Artifacts and Registered Models, you can effectively manage and categorize various versions of your models from different runs in W&B.\\n')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations = generate_questions([documents[0]], n_questions=3, n_generations=5)\n",
    "parse_generation(generations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Artifact generated_examples>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_generations = []\n",
    "generations = generate_questions(documents, n_questions=3, n_generations=5)\n",
    "for generation in generations:\n",
    "    context, question, answer = parse_generation(generation)\n",
    "    parsed_generations.append({\"context\": context, \"question\": question, \"answer\": answer})\n",
    "\n",
    "# let's convert parsed_generations to a pandas dataframe and save it locally\n",
    "df = pd.DataFrame(parsed_generations)\n",
    "df.to_csv('generated_examples.csv', index=False)\n",
    "\n",
    "# log df as a table to W&B for interactive exploration\n",
    "wandb.log({\"generated_examples\": wandb.Table(dataframe=df)})\n",
    "\n",
    "# log csv file as an artifact to W&B for later use\n",
    "artifact = wandb.Artifact(\"generated_examples\", type=\"dataset\")\n",
    "artifact.add_file(\"generated_examples.csv\")\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-frog-8</strong> at: <a href='https://wandb.ai/kmirijan/llmapps/runs/2ugzwzbk' target=\"_blank\">https://wandb.ai/kmirijan/llmapps/runs/2ugzwzbk</a><br/> View project at: <a href='https://wandb.ai/kmirijan/llmapps' target=\"_blank\">https://wandb.ai/kmirijan/llmapps</a><br/>Synced 4 W&B file(s), 0 media file(s), 7 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241129_090511-2ugzwzbk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wandb_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
