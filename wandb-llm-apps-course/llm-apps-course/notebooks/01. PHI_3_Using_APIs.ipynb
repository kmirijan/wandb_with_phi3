{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the HuggingFace API and Simple Generation\n",
    "\n",
    "We will explore HuggingFace models API to generate text, specificalyy with Phi-3.5-mini\n",
    "\n",
    "<!--- @wandbcode{llmapps-intro} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import wandb\n",
    "import torch\n",
    "from pprint import pprint\n",
    "from getpass import getpass\n",
    "from wandb.integration.openai import autolog\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.20s/it]\n",
      "You shouldn't move a model when it is dispatched on multiple devices.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "# chat_checkpoint = \"microsoft/phi-2\"\n",
    "chat_checkpoint = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(chat_checkpoint)\n",
    "\n",
    "chat_model = AutoModelForCausalLM.from_pretrained(\n",
    "        chat_checkpoint,\n",
    "        device_map=\"cuda\",\n",
    "        torch_dtype=\"auto\",\n",
    "        quantization_config=bnb_config, \n",
    "        use_flash_attention_2=False,\n",
    "        trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "chat_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's enable W&B autologging to track our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kmirijan/wandb/wandb-llm-apps-course/llm-apps-course/notebooks/wandb/run-20241111_104831-qco3sbbh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kmirijan/llmapps/runs/qco3sbbh' target=\"_blank\">likely-snow-6</a></strong> to <a href='https://wandb.ai/kmirijan/llmapps' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kmirijan/llmapps' target=\"_blank\">https://wandb.ai/kmirijan/llmapps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kmirijan/llmapps/runs/qco3sbbh' target=\"_blank\">https://wandb.ai/kmirijan/llmapps/runs/qco3sbbh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start logging to W&B\n",
    "autolog({\"project\":\"llmapps\", \"job_type\": \"introduction\"})\n",
    "wandb.config.update({\n",
    "    \"model_name\": \"phi-3.5-mini-instruct\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1334, 5861, 669, 3457, 2129, 338, 29663, 29991]\n",
      "Weights & Biases is awesome!\n"
     ]
    }
   ],
   "source": [
    "# encoding = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "text = \"Weights & Biases is awesome!\"\n",
    "encoded_text = tokenizer.encode(text)\n",
    "print(encoded_text)\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can decode the tokens one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\tWe\n",
      "5861\tights\n",
      "669\t&\n",
      "3457\tBi\n",
      "2129\tases\n",
      "338\tis\n",
      "29663\tawesome\n",
      "29991\t!\n"
     ]
    }
   ],
   "source": [
    "for token_id in encoded_text:\n",
    "    print(f\"{token_id}\\t{tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how the leading tokens contain spacing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample some text from the model. For this, let's create a wrapper function around the temperature parameters.\n",
    "Higher temperature will result in more random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature(temp):\n",
    "  \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "  response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Say something about Weights & Biases\",\n",
    "    max_tokens=50,\n",
    "    temperature=temp,\n",
    "  )\n",
    "  return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature_phi(chat_model, tokenizer, temp):\n",
    "  \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "  prompt = \"Say something about Weights & Biases.\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "  outputs = chat_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask = inputs['attention_mask'],\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    do_sample = True,\n",
    "    temperature=temp,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "  )\n",
    "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  generated_text = generated_text[len(prompt):].strip()\n",
    "  wandb.log({ \"prompt\": prompt, \n",
    "              \"generated_text\": generated_text, \n",
    "              \"temperature\": temp,\n",
    "              \"top_p\":topp,\n",
    "              \"top_k\":topk,\n",
    "              \"tokens_used\": len(outputs[0]) })\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TEMP: 0.0001, GENERATION: Assistant: Weights & Biases (W&B) is an '\n",
      " 'open-source platform designed to help machine learning practitioners and '\n",
      " 'researchers monitor and visualize the training process of their models. It '\n",
      " 'provides a range of tools to track experiments, log metrics, and analyze '\n",
      " 'model performance in real-time. Here are some key features and benefits of '\n",
      " 'using Weights & Biases:\\n'\n",
      " '\\n'\n",
      " '1. Real-time Monitoring:')\n",
      "('TEMP: 0.01, GENERATION: Assistant: Weights & Biases (wandb) is a platform '\n",
      " 'for machine learning experiment tracking and visualization. It allows '\n",
      " 'researchers and data scientists to log, track, and visualize metrics during '\n",
      " 'the training of machine learning models. Here are some key features and '\n",
      " 'benefits of using Weights & Biases:\\n'\n",
      " '\\n'\n",
      " \"1. **Real-time Monitoring**: You can monitor your model's training progress \"\n",
      " 'in real-')\n",
      "('TEMP: 0.1, GENERATION: Assistant: Weights & Biases (W&B) is an open-source '\n",
      " 'platform for machine learning experiment tracking, data exploration, and '\n",
      " \"collaborative project management. It's designed to help data scientists and \"\n",
      " 'machine learning practitioners monitor and visualize the progress of their '\n",
      " 'models during training, experiment with hyperparameters, and share their '\n",
      " 'findings with the community. W&B integrates with popular deep learning '\n",
      " 'frameworks like Tensor')\n",
      "('TEMP: 0.3, GENERATION: Assistant: Weights & Biases (wandb) is an open-source '\n",
      " 'platform for machine learning experiment tracking and visualization. It '\n",
      " 'allows researchers and data scientists to log, track, and visualize metrics '\n",
      " 'during the training of machine learning models. Here are some key features '\n",
      " 'and benefits of using Weights & Biases:\\n'\n",
      " '\\n'\n",
      " '1. **Real-time Tracking**: It provides real-time tracking of metrics such as')\n",
      "('TEMP: 0.5, GENERATION: Assistant: Weights & Biases (wandb) is a platform for '\n",
      " 'machine learning experiment tracking and visualization. It allows '\n",
      " 'researchers and data scientists to log, track, and visualize the progress of '\n",
      " 'their machine learning models and experiments. With wandb, users can monitor '\n",
      " 'metrics like loss and accuracy, visualize data, and perform hyperparameter '\n",
      " 'tuning. It also provides features like model comparison, model sharing, and '\n",
      " 'community support.')\n",
      "(\"TEMP: 1, GENERATION: I'm not sure how to integrate it, can you guide me on \"\n",
      " 'that? Weights & Biases (wandb) is a platform that tracks experiments, '\n",
      " \"visualizes data, and shares your machine learning models. It's especially \"\n",
      " 'useful for monitoring the performance of different configurations during '\n",
      " 'hyperparameter optimization.\\n'\n",
      " '\\n'\n",
      " \"Here's a brief guide on how to get started with Weights & Biases for your \"\n",
      " 'PyTorch Lightning model:')\n"
     ]
    }
   ],
   "source": [
    "for temp in [0.0001, 0.01, 0.1, 0.3, 0.5, 1]:\n",
    "  pprint(f'TEMP: {temp}, GENERATION: {generate_with_temperature_phi(chat_model, tokenizer, temp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried a bunch of stuff here, it didn't work out well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the [`top_p` parameter](https://platform.openai.com/docs/api-reference/completions/create#completions/create-top_p) to control the diversity of the generated text. This parameter controls the cumulative probability of the next token. For example, if `top_p=0.9`, the model will pick the next token from the top 90% most likely tokens. The higher the `top_p` the more likely the model will pick a token that it hasn't seen before. You should only use one of `temperature` or `top_p` at a given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_topp(topp):\n",
    "  \"Generate text with a given top-p, higher top-p means more randomness\"\n",
    "  response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Say something about Weights & Biases\",\n",
    "    max_tokens=50,\n",
    "    top_p=topp,\n",
    "    )\n",
    "  return response.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_topp_phi(chat_model, tokenizer, topp):\n",
    "  \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "  prompt = \"Say something about Weights & Biases.\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "  outputs = chat_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask = inputs['attention_mask'],\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    do_sample = True,\n",
    "    top_p=topp,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "  )\n",
    "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  generated_text = generated_text[len(prompt):].strip()\n",
    "  wandb.log({ \"prompt\": prompt, \n",
    "              \"generated_text\": generated_text, \n",
    "              \"temperature\": temp,\n",
    "              \"top_p\":topp,\n",
    "              \"top_k\":topk,\n",
    "              \"tokens_used\": len(outputs[0]) })\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TOP_P: 0.7, GENERATION: Assistant: Weights & Biases (W&B) is a platform for '\n",
      " 'machine learning experimentation that is particularly useful for tracking '\n",
      " 'and visualizing the training process of neural networks. It offers several '\n",
      " 'features that are beneficial for researchers and data scientists:\\n'\n",
      " '\\n'\n",
      " '1. **Visualization**: W&B allows you to visualize metrics such as loss and '\n",
      " 'accuracy in real-time as your model trains, which can help in')\n",
      "('TOP_P: 0.8, GENERATION: Assistant: Weights & Biases (wandb) is a '\n",
      " 'comprehensive platform for tracking experiments, visualizing metrics, and '\n",
      " 'managing datasets during the machine learning model training process. It is '\n",
      " 'particularly useful for:\\n'\n",
      " '\\n'\n",
      " '1. **Experiment Tracking**: It allows you to log every aspect of your '\n",
      " 'experiment, from hyperparameters to model performance, to easily compare '\n",
      " 'different runs.\\n'\n",
      " '\\n'\n",
      " '2. **Visualization**: With wand')\n",
      "('TOP_P: 0.9, GENERATION: Chatbot: Certainly! Weights & Biases (W&B) is a '\n",
      " 'platform designed to simplify the process of tracking and visualizing '\n",
      " 'machine learning experiments. Here are key features that you might find '\n",
      " 'useful:\\n'\n",
      " '\\n'\n",
      " '1. **Experiment Tracking**: W&B allows you to track changes to the '\n",
      " \"hyperparameters and visualize the impact they have on the model's \"\n",
      " 'performance, providing a clear overview of your')\n",
      "('TOP_P: 1, GENERATION: Human: Why is Weights & Biases not good for deep '\n",
      " 'learning?\\n'\n",
      " '\\n'\n",
      " 'Assistant: Weights & Biases is a popular tool for tracking and visualizing '\n",
      " 'the progress of model training. It is often used in the context of deep '\n",
      " 'learning, as it provides insights into the performance of models during '\n",
      " 'training. However, it may not always be the best solution for deep learning '\n",
      " 'models for several reasons:\\n'\n",
      " '\\n'\n",
      " '1. Limited Custom')\n"
     ]
    }
   ],
   "source": [
    "for topp in [0.7, 0.8, 0.9, 1]:\n",
    "  pprint(f'TOP_P: {topp}, GENERATION: {generate_with_topp_phi(chat_model, tokenizer, topp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now also try TOP_K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_topk_phi(chat_model, tokenizer, topk):\n",
    "  \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "  prompt = \"Say something about Weights & Biases.\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "  outputs = chat_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask = inputs['attention_mask'],\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    do_sample = True,\n",
    "    top_k=topk,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "  )\n",
    "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  generated_text = generated_text[len(prompt):].strip()\n",
    "  wandb.log({ \"prompt\": prompt, \n",
    "              \"generated_text\": generated_text, \n",
    "              \"temperature\": temp,\n",
    "              \"top_p\":topp,\n",
    "              \"top_k\":topk,\n",
    "              \"tokens_used\": len(outputs[0]) })\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"TOP_K: 10, GENERATION: Hey there! I'd be happy to chat about Weights & \"\n",
      " 'Biases and its features.\\n'\n",
      " '\\n'\n",
      " 'Weights & Biases (W&B) is a powerful tool for AI developers and researchers. '\n",
      " \"It's an open-source platform that integrates deep learning experiment \"\n",
      " 'management, data logging, and the tracking of metrics during model training. '\n",
      " 'Here are a few key features:\\n'\n",
      " '\\n'\n",
      " '1. **Visualization**: W')\n",
      "(\"TOP_K: 20, GENERATION: But I've got another part coming your way - we're \"\n",
      " 'delving deep into the heart of a project where a critical functionality '\n",
      " \"isn't quite up to par. Here's where the W&B's playtime shines, acting like a \"\n",
      " 'traffic reporter for our code, spotting speedbumps and guiding us through '\n",
      " 'the optimization maze.\\n'\n",
      " '\\n'\n",
      " \"So, let's roll up our sleeves, dive\")\n",
      "('TOP_K: 30, GENERATION: Assistant: Weights & Biases (wandb) is a powerful '\n",
      " 'tool for experiment tracking and visualization used in machine learning and '\n",
      " 'data science projects. It provides a platform where you can store and '\n",
      " \"visualize the metrics of your models as you train them. Here's a brief \"\n",
      " 'overview of its features and functionalities:\\n'\n",
      " '\\n'\n",
      " '1. **Experiment Tracking**: With wandb, you can log every experiment, making '\n",
      " 'it')\n",
      "(\"TOP_K: 50, GENERATION: Sure, let's discuss each of these points in turn:\\n\"\n",
      " '\\n'\n",
      " '1. **Rounding Errors in Large Arrays:**\\n'\n",
      " 'When working with large arrays, rounding errors can occur due to the '\n",
      " 'limitations of floating-point arithmetic on computers. Floating-point '\n",
      " 'numbers are represented in a fixed number of significant digits, which means '\n",
      " 'that not all numbers can be represented exactly. When you perform arithmetic '\n",
      " 'operations on these numbers,')\n",
      "('TOP_K: 70, GENERATION: Assistant: Cleanlab_Cleanlab is a Python library that '\n",
      " 'provides functions and tools for cleaning and labeling noisy datasets, often '\n",
      " 'used in the context of semi-supervised learning. It implements the CLR '\n",
      " '(Cleanlab) algorithm that aims to identify and label noisy (mislabelled or '\n",
      " 'unreliable) instances within datasets, which is particularly useful when you '\n",
      " 'have incomplete or uncertain labels.')\n",
      "('TOP_K: 100, GENERATION: Chatmosphere: Certainly, Weights & Biases (wandb) is '\n",
      " 'a powerful platform for machine learning experiment tracking, data '\n",
      " \"visualization, and collaborative project management. Here's an overview:\\n\"\n",
      " '\\n'\n",
      " '1. **Experiment Tracking**: It allows you to monitor and record each step of '\n",
      " 'your ML project. You can define metrics, hyperparameters, datasets, and '\n",
      " 'model configurations for every experiment.')\n"
     ]
    }
   ],
   "source": [
    "for topk in [10, 20, 30, 50, 70, 100]:\n",
    "  pprint(f'TOP_K: {topk}, GENERATION: {generate_with_topp_phi(chat_model, tokenizer, topk)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try all 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_all_samp_phi(chat_model, tokenizer, temp, topp, topk):\n",
    "  \"Generate text with a given temperature, higher temperature means more randomness\"\n",
    "  prompt = \"Say something about Weights & Biases.\"\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "  outputs = chat_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask = inputs['attention_mask'],\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    do_sample = True,\n",
    "    temperature=temp,\n",
    "    top_p=topp,\n",
    "    top_k=topk,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "  )\n",
    "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "  wandb.log({ \"prompt\": prompt, \n",
    "              \"generated_text\": generated_text, \n",
    "              \"temperature\": temp,\n",
    "              \"top_p\":topp,\n",
    "              \"top_k\":topk,\n",
    "              \"tokens_used\": len(outputs[0]) })\n",
    "\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP_P: 0.9, GENERATION: Assistant: Weights & Biases (W&B) is a scalable experiment-tracking platform for machine learning research and development. It's designed to help researchers and data scientists monitor and visualize the training process of their models in real-time. Here are some key features and benefits of using Weights & Biases:\n",
      "\n",
      "1. **Real-time Monitoring**: W&B allows you\n"
     ]
    }
   ],
   "source": [
    "temp = 0.3\n",
    "topp = 0.9\n",
    "topk = 70\n",
    "print(f'TOP_P: {topp}, GENERATION: {generate_with_all_samp_phi(chat_model, tokenizer, temp, topp, topk)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to chat mode and see how the model responds to our messages. We have some control over the model's response by passing a `system-role`, here we can steer to model to adhere to a certain behaviour.\n",
    "\n",
    "> We are using `gpt-3.5-turbo`, this model is faster and cheaper than `davinci-003`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Say something about Weights & Biases\"}\n",
    "]\n",
    "\n",
    "def format_conversation(history):\n",
    "    formatted_history = \"\"\n",
    "    for message in history:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            formatted_history += f\"System: {message['content']}\\n\"\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            formatted_history += f\"User: {message['content']}\\n\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            formatted_history += f\"Assistant: {message['content']}\\n\"\n",
    "    return formatted_history\n",
    "\n",
    "formatted_history = format_conversation(conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Weights & Biases (W&B) is an open-source platform for machine learning experimentation and collaboration. It provides a user-friendly interface to manage and visualize machine learning models, datasets, and experiments. W&B allows users to track and compare different model versions, monitor training progress, and analyze model performance. It also supports hyperparameter tuning, model\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(formatted_history, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "outputs = chat_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=100\n",
    ")\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "assistant_response = generated_text.split(\"Assistant:\")[-1].strip()\n",
    "wandb.log({ \"generated_text\": generated_text, \n",
    "            \"assistant_response\": assistant_response,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\":topp,\n",
    "            \"top_k\":topk,\n",
    "            \"tokens_used\": len(outputs[0]) })\n",
    "print(f\"Assistant: {assistant_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the response is a JSON object with relevant information about the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>temperature</td><td>▁▁▂▃▄███████████▃▃</td></tr><tr><td>tokens_used</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>top_k</td><td>▆▆▆▆▆▆▆▆▆▆▁▂▃▄▆█▆▆</td></tr><tr><td>top_p</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▃▄▆█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>assistant_response</td><td>Weights & Biases (W&...</td></tr><tr><td>generated_text</td><td>System: You are a he...</td></tr><tr><td>prompt</td><td>Say something about ...</td></tr><tr><td>temperature</td><td>0.3</td></tr><tr><td>tokens_used</td><td>100</td></tr><tr><td>top_k</td><td>70</td></tr><tr><td>top_p</td><td>0.9</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-snow-6</strong> at: <a href='https://wandb.ai/kmirijan/llmapps/runs/qco3sbbh' target=\"_blank\">https://wandb.ai/kmirijan/llmapps/runs/qco3sbbh</a><br/> View project at: <a href='https://wandb.ai/kmirijan/llmapps' target=\"_blank\">https://wandb.ai/kmirijan/llmapps</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241111_104831-qco3sbbh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wandb_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
